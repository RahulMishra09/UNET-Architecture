{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport os\nimport glob\nimport nibabel as nib\nfrom sklearn.model_selection import train_test_split\nfrom scipy.ndimage import zoom\n\n# Disable XLA compilation for faster startup\ntf.config.optimizer.set_jit(False)\n\n# Set memory growth to avoid GPU memory issues\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(f\"GPU memory growth setting failed: {e}\")\n\n# Enable mixed precision for better performance\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\n\nclass OptimizedSEBlock3D(layers.Layer):\n    \"\"\"Lightweight 3D Squeeze-and-Excitation Block\"\"\"\n    \n    def __init__(self, channels, reduction=16, **kwargs):\n        super(OptimizedSEBlock3D, self).__init__(**kwargs)\n        self.channels = channels\n        self.reduction = reduction\n        \n        # Use smaller reduction ratio for efficiency\n        reduced_channels = max(channels // reduction, 4)\n        \n        # Global average pooling\n        self.global_avg_pool = layers.GlobalAveragePooling3D(keepdims=True)\n        \n        # Simplified squeeze and excitation\n        self.squeeze = layers.Dense(reduced_channels, activation='relu')\n        self.excitation = layers.Dense(channels, activation='sigmoid')\n        \n    def call(self, inputs):\n        # Squeeze: Global average pooling\n        squeezed = self.global_avg_pool(inputs)\n        squeezed = tf.reshape(squeezed, [-1, self.channels])\n        \n        # Excitation: Dense layers\n        excited = self.squeeze(squeezed)\n        excited = self.excitation(excited)\n        excited = tf.reshape(excited, [-1, 1, 1, 1, self.channels])\n        \n        # Scale original input\n        return inputs * excited\n\n\nclass LightweightParallelConvBlock(layers.Layer):\n    \"\"\"Lightweight Parallel Convolution Block - simplified for performance\"\"\"\n    \n    def __init__(self, filters, **kwargs):\n        super(LightweightParallelConvBlock, self).__init__(**kwargs)\n        self.filters = filters\n        \n        # Use only 1x1 and 3x3 convolutions (remove 5x5 for speed)\n        self.conv_1x1 = layers.Conv3D(\n            filters // 2, \n            kernel_size=(1, 1, 1), \n            padding='same',\n            kernel_regularizer=keras.regularizers.l2(0.01)\n        )\n        self.conv_3x3 = layers.Conv3D(\n            filters // 2, \n            kernel_size=(3, 3, 3), \n            padding='same',\n            kernel_regularizer=keras.regularizers.l2(0.01)\n        )\n        \n        # Batch normalization instead of layer normalization for speed\n        self.batch_norm = layers.BatchNormalization()\n        self.leaky_relu = layers.LeakyReLU(alpha=0.1)\n        self.dropout = layers.Dropout(rate=0.1)  # Reduced dropout\n        self.max_pool = layers.MaxPooling3D(pool_size=(2, 2, 2))\n        \n    def call(self, inputs, training=None):\n        # Parallel branches (only 1x1 and 3x3)\n        branch_1x1 = self.conv_1x1(inputs)\n        branch_3x3 = self.conv_3x3(inputs)\n        \n        # Concatenate branches\n        concat = layers.concatenate([branch_1x1, branch_3x3], axis=-1)\n        \n        # Apply normalization and activation\n        normalized = self.batch_norm(concat, training=training)\n        activated = self.leaky_relu(normalized)\n        dropped = self.dropout(activated, training=training)\n        \n        # Max pooling for encoder\n        pooled = self.max_pool(dropped)\n        \n        return dropped, pooled\n\n\nclass OptimizedEncoderBlock(layers.Layer):\n    \"\"\"Optimized Encoder block with single convolution for speed\"\"\"\n    \n    def __init__(self, filters, **kwargs):\n        super(OptimizedEncoderBlock, self).__init__(**kwargs)\n        self.filters = filters\n        \n        # Single convolution instead of double for speed\n        self.conv = layers.Conv3D(\n            filters, \n            kernel_size=(3, 3, 3), \n            padding='same',\n            kernel_regularizer=keras.regularizers.l2(0.01)\n        )\n        \n        # Batch normalization for speed\n        self.batch_norm = layers.BatchNormalization()\n        self.leaky_relu = layers.LeakyReLU(alpha=0.1)\n        self.dropout = layers.Dropout(rate=0.1)\n        \n        # Lightweight SE attention\n        self.se_block = OptimizedSEBlock3D(filters)\n        \n        # Max pooling\n        self.max_pool = layers.MaxPooling3D(pool_size=(2, 2, 2))\n        \n    def call(self, inputs, training=None):\n        # Single convolution\n        x = self.conv(inputs)\n        x = self.batch_norm(x, training=training)\n        x = self.leaky_relu(x)\n        x = self.dropout(x, training=training)\n        \n        # SE attention\n        x = self.se_block(x)\n        \n        # Store skip connection before pooling\n        skip = x\n        \n        # Max pooling\n        pooled = self.max_pool(x)\n        \n        return skip, pooled\n\n\nclass WeightedDiceLoss(keras.losses.Loss):\n    \"\"\"Optimized Weighted Dice Loss\"\"\"\n    \n    def __init__(self, class_weights=None, smooth=1e-6, **kwargs):\n        super(WeightedDiceLoss, self).__init__(**kwargs)\n        \n        if class_weights is None:\n            self.class_weights = tf.constant([1.0, 2.0, 2.0, 3.0], dtype=tf.float32)\n        else:\n            self.class_weights = tf.constant(class_weights, dtype=tf.float32)\n        \n        self.smooth = smooth\n        \n    def call(self, y_true, y_pred):\n        # Ensure float32 for mixed precision\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        \n        # Calculate dice coefficient for each class\n        dice_scores = []\n        \n        for i in range(4):  # 4 classes\n            y_true_class = y_true[..., i]\n            y_pred_class = y_pred[..., i]\n            \n            intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1,2,3])\n            union = tf.reduce_sum(y_true_class, axis=[1,2,3]) + tf.reduce_sum(y_pred_class, axis=[1,2,3])\n            \n            dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n            dice_scores.append(tf.reduce_mean(dice))\n        \n        # Convert to tensor\n        dice_scores = tf.stack(dice_scores)\n        \n        # Apply class weights\n        weighted_dice = dice_scores * self.class_weights\n        \n        # Return negative weighted average (loss to minimize)\n        return -tf.reduce_mean(weighted_dice)\n\n\ndef create_optimized_latup_net(input_shape=(64, 64, 64, 3)):  # Reduced input size\n    \"\"\"\n    Create Performance-Optimized LATUP-Net model\n    \n    Args:\n        input_shape: Input shape (height, width, depth, channels) - reduced to 64^3\n    \n    Returns:\n        Keras model\n    \"\"\"\n    inputs = keras.Input(shape=input_shape)\n    \n    # Input normalization\n    x = layers.Lambda(lambda x: tf.nn.sigmoid(x))(inputs)\n    \n    # Encoder path with reduced complexity\n    # Level 1: 64x64x64 -> 32x32x32\n    pc_block = LightweightParallelConvBlock(16)  # Reduced filters\n    skip1, x = pc_block(x)  # skip1: (64,64,64,16), x: (32,32,32,16)\n    \n    # Level 2: 32x32x32 -> 16x16x16  \n    enc_block2 = OptimizedEncoderBlock(32)  # Reduced filters\n    skip2, x = enc_block2(x)  # skip2: (32,32,32,32), x: (16,16,16,32)\n    \n    # Level 3: 16x16x16 -> 8x8x8\n    enc_block3 = OptimizedEncoderBlock(64)  # Reduced filters\n    skip3, x = enc_block3(x)  # skip3: (16,16,16,64), x: (8,8,8,64)\n    \n    # Bottleneck: 8x8x8 -> 4x4x4\n    x = layers.Conv3D(128, (3, 3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(0.01))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(alpha=0.1)(x)\n    x = layers.Dropout(0.1)(x)\n    x = OptimizedSEBlock3D(128)(x)\n    \n    # Pool to bottleneck\n    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)  # (4,4,4,128)\n    \n    # Decoder path with corrected skip connections\n    # Level 3: 4x4x4 -> 8x8x8, connect with skip3 (16,16,16,64)\n    x = layers.UpSampling3D(size=(2, 2, 2))(x)  # (8,8,8,128)\n    # Downsample skip3 to match: (16,16,16,64) -> (8,8,8,64)\n    skip3_downsampled = layers.MaxPooling3D(pool_size=(2, 2, 2))(skip3)\n    x = layers.concatenate([x, skip3_downsampled], axis=-1)  # (8,8,8,128+64=192)\n    x = layers.Conv3D(64, (1, 1, 1), padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(alpha=0.1)(x)\n    x = layers.Dropout(0.1)(x)\n    x = OptimizedSEBlock3D(64)(x)\n    \n    # Level 2: 8x8x8 -> 16x16x16, connect with skip2 (32,32,32,32)\n    x = layers.UpSampling3D(size=(2, 2, 2))(x)  # (16,16,16,64)\n    # Downsample skip2 to match: (32,32,32,32) -> (16,16,16,32)\n    skip2_downsampled = layers.MaxPooling3D(pool_size=(2, 2, 2))(skip2)\n    x = layers.concatenate([x, skip2_downsampled], axis=-1)  # (16,16,16,64+32=96)\n    x = layers.Conv3D(32, (1, 1, 1), padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(alpha=0.1)(x)\n    x = layers.Dropout(0.1)(x)\n    x = OptimizedSEBlock3D(32)(x)\n    \n    # Level 1: 16x16x16 -> 32x32x32, connect with skip1 (64,64,64,16)\n    x = layers.UpSampling3D(size=(2, 2, 2))(x)  # (32,32,32,32)\n    # Downsample skip1 to match: (64,64,64,16) -> (32,32,32,16)\n    skip1_downsampled = layers.MaxPooling3D(pool_size=(2, 2, 2))(skip1)\n    x = layers.concatenate([x, skip1_downsampled], axis=-1)  # (32,32,32,32+16=48)\n    x = layers.Conv3D(16, (1, 1, 1), padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(alpha=0.1)(x)\n    x = layers.Dropout(0.1)(x)\n    x = OptimizedSEBlock3D(16)(x)\n    \n    # Final upsampling: 32x32x32 -> 64x64x64\n    x = layers.UpSampling3D(size=(2, 2, 2))(x)  # (64,64,64,16)\n    \n    # Final classification layer\n    outputs = layers.Conv3D(\n        4, \n        kernel_size=1, \n        activation='softmax',\n        dtype=tf.float32  # Ensure float32 output for mixed precision\n    )(x)\n    \n    # Create model\n    model = keras.Model(inputs=inputs, outputs=outputs, name='Optimized-LATUP-Net')\n    \n    return model\n\n\n# Custom Keras Metrics for dice score monitoring (simplified)\nclass FastDiceScore(keras.metrics.Metric):\n    \"\"\"Fast Keras metric for overall dice score\"\"\"\n    \n    def __init__(self, name='dice_score', **kwargs):\n        super(FastDiceScore, self).__init__(name=name, **kwargs)\n        self.dice_sum = self.add_weight(name='dice_sum', initializer='zeros', dtype=tf.float32)\n        self.count = self.add_weight(name='count', initializer='zeros', dtype=tf.float32)\n        self.smooth = 1e-6\n        \n    def update_state(self, y_true, y_pred, sample_weight=None):\n        # Convert to float32\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        \n        # Simplified dice calculation\n        intersection = tf.reduce_sum(y_true * y_pred)\n        union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n        \n        self.dice_sum.assign_add(dice)\n        self.count.assign_add(1)\n        \n    def result(self):\n        return self.dice_sum / self.count\n    \n    def reset_state(self):\n        self.dice_sum.assign(0)\n        self.count.assign(0)\n\n\ndef resize_volume_fast(volume, target_shape=(64, 64, 64)):\n    \"\"\"Fast volume resizing using nearest neighbor\"\"\"\n    current_shape = volume.shape\n    zoom_factors = [target_shape[i] / current_shape[i] for i in range(3)]\n    return zoom(volume, zoom_factors, order=0)  # order=0 for nearest neighbor (faster)\n\n\ndef load_brats_case_optimized(case_path):\n    \"\"\"\n    Optimized BraTS case loading with reduced resolution\n    \n    Args:\n        case_path: Path to case directory\n    \n    Returns:\n        input_volume: (64, 64, 64, 3) - T1ce, T2, FLAIR (reduced size)\n        mask: (64, 64, 64) - segmentation mask (reduced size)\n    \"\"\"\n    case_name = os.path.basename(case_path)\n    \n    # Load modalities\n    t1ce_path = os.path.join(case_path, f\"{case_name}_t1ce.nii\")\n    t2_path = os.path.join(case_path, f\"{case_name}_t2.nii\")\n    flair_path = os.path.join(case_path, f\"{case_name}_flair.nii\")\n    seg_path = os.path.join(case_path, f\"{case_name}_seg.nii\")\n    \n    # Check if all files exist\n    if not all(os.path.exists(path) for path in [t1ce_path, t2_path, flair_path, seg_path]):\n        print(f\"Missing files for case: {case_name}\")\n        return None, None\n    \n    try:\n        # Load volumes\n        t1ce = nib.load(t1ce_path).get_fdata()\n        t2 = nib.load(t2_path).get_fdata()\n        flair = nib.load(flair_path).get_fdata()\n        seg = nib.load(seg_path).get_fdata()\n        \n        # Resize to smaller target shape for performance\n        t1ce = resize_volume_fast(t1ce, target_shape=(64, 64, 64))\n        t2 = resize_volume_fast(t2, target_shape=(64, 64, 64))\n        flair = resize_volume_fast(flair, target_shape=(64, 64, 64))\n        seg = resize_volume_fast(seg, target_shape=(64, 64, 64))\n        \n        # Fast normalization\n        def normalize_fast(volume):\n            volume = volume.astype(np.float32)\n            if volume.max() > 0:\n                volume = volume / volume.max()\n            return volume\n        \n        t1ce = normalize_fast(t1ce)\n        t2 = normalize_fast(t2)\n        flair = normalize_fast(flair)\n        \n        # Stack modalities\n        input_volume = np.stack([t1ce, t2, flair], axis=-1)\n        \n        # Convert segmentation labels\n        seg_processed = np.zeros_like(seg)\n        seg_processed[seg == 1] = 1  # necrotic core\n        seg_processed[seg == 2] = 2  # peritumoral edema  \n        seg_processed[seg == 4] = 3  # enhancing tumor\n        \n        return input_volume.astype(np.float32), seg_processed.astype(np.int32)\n    \n    except Exception as e:\n        print(f\"Error loading case {case_path}: {e}\")\n        return None, None\n\n\ndef create_optimized_brats_generator(case_paths, batch_size=1, shuffle=True):\n    \"\"\"Optimized data generator for BraTS dataset\"\"\"\n    \n    def data_generator():\n        indices = np.arange(len(case_paths))\n        if shuffle:\n            np.random.shuffle(indices)\n        \n        for i in range(0, len(indices), batch_size):\n            batch_indices = indices[i:i+batch_size]\n            batch_x = []\n            batch_y = []\n            \n            for idx in batch_indices:\n                case_path = case_paths[idx]\n                x, y = load_brats_case_optimized(case_path)\n                if x is not None and y is not None:\n                    batch_x.append(x)\n                    # Convert to one-hot encoding\n                    y_onehot = tf.one_hot(y, depth=4).numpy()\n                    batch_y.append(y_onehot)\n            \n            if batch_x:\n                yield np.array(batch_x), np.array(batch_y)\n    \n    return data_generator\n\n\ndef create_optimized_tf_dataset(generator_func, num_cases, batch_size=1):\n    \"\"\"Create optimized TensorFlow dataset from generator\"\"\"\n    output_signature = (\n        tf.TensorSpec(shape=(batch_size, 64, 64, 64, 3), dtype=tf.float32),  # Reduced size\n        tf.TensorSpec(shape=(batch_size, 64, 64, 64, 4), dtype=tf.float32)\n    )\n    \n    dataset = tf.data.Dataset.from_generator(\n        generator_func,\n        output_signature=output_signature\n    )\n    \n    # Optimize dataset performance\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    dataset = dataset.cache()  # Cache for faster access\n    \n    return dataset\n\n\ndef compile_optimized_model(model):\n    \"\"\"Compile model with optimized settings\"\"\"\n    \n    # Use simpler metrics for faster training\n    metrics = [\n        'accuracy',\n        FastDiceScore(name='dice_score')\n    ]\n    \n    # Compile with optimized settings\n    model.compile(\n        optimizer=keras.optimizers.Adam(\n            learning_rate=1e-3,  # Higher learning rate for faster convergence\n            clipnorm=1.0  # Gradient clipping for stability\n        ),\n        loss=WeightedDiceLoss(),\n        metrics=metrics\n    )\n    \n    return model\n\n\ndef train_optimized_model(model, train_data, val_data=None, epochs=50):\n    \"\"\"Train model with optimized settings\"\"\"\n    \n    # Optimized callbacks\n    callbacks = [\n        keras.callbacks.ModelCheckpoint(\n            'best_optimized_latup.weights.h5',\n            monitor='val_dice_score' if val_data else 'dice_score',\n            save_best_only=True,\n            save_weights_only=True,\n            mode='max',\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_dice_score' if val_data else 'dice_score',\n            factor=0.5,\n            patience=5,  # Reduced patience\n            min_lr=1e-6,\n            mode='max',\n            verbose=1\n        ),\n        keras.callbacks.EarlyStopping(\n            monitor='val_dice_score' if val_data else 'dice_score',\n            patience=10,  # Reduced patience\n            restore_best_weights=True,\n            mode='max',\n            verbose=1\n        )\n    ]\n    \n    print(\"ğŸš€ Starting Optimized LATUP-Net training...\")\n    print(\"âœ… Performance optimizations applied:\")\n    print(\"   â€¢ Reduced model complexity\")\n    print(\"   â€¢ Mixed precision training\") \n    print(\"   â€¢ Smaller input size (64Â³)\")\n    print(\"   â€¢ Batch normalization\")\n    print(\"   â€¢ Simplified SE blocks\")\n    print(\"   â€¢ XLA disabled for faster startup\")\n    print(\"   â€¢ GPU memory growth enabled\")\n    \n    history = model.fit(\n        train_data,\n        validation_data=val_data,\n        epochs=epochs,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    return history\n\n\n# Main optimized training script\nif __name__ == \"__main__\":\n    print(\"ğŸ”§ Performance-Optimized LATUP-Net\")\n    print(\"=\" * 50)\n    \n    # Create optimized model\n    print(\"Creating optimized model...\")\n    model = create_optimized_latup_net(input_shape=(64, 64, 64, 3))\n    model = compile_optimized_model(model)\n    \n    # Test model creation\n    print(\"Testing model...\")\n    dummy_input = tf.random.normal((1, 64, 64, 64, 3))\n    dummy_output = model(dummy_input)\n    print(f\"âœ… Model test passed! Output shape: {dummy_output.shape}\")\n    \n    # Print model summary\n    model.summary()\n    \n    # Data directory\n    DATA_DIR = \"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n    \n    if not os.path.exists(DATA_DIR):\n        print(f\"Data directory not found: {DATA_DIR}\")\n        print(\"Testing with dummy data...\")\n        \n        # Create dummy data for testing\n        dummy_train_data = tf.data.Dataset.from_tensor_slices((\n            tf.random.normal((4, 64, 64, 64, 3)),\n            tf.random.uniform((4, 64, 64, 64, 4), maxval=1.0)\n        )).batch(1).prefetch(tf.data.AUTOTUNE)\n        \n        dummy_val_data = tf.data.Dataset.from_tensor_slices((\n            tf.random.normal((2, 64, 64, 64, 3)),\n            tf.random.uniform((2, 64, 64, 64, 4), maxval=1.0)\n        )).batch(1).prefetch(tf.data.AUTOTUNE)\n        \n        # Test training\n        print(\"Testing training with dummy data...\")\n        history = train_optimized_model(model, dummy_train_data, dummy_val_data, epochs=2)\n        print(\"âœ… Optimized training test completed!\")\n    \n    else:\n        # Load real data\n        print(\"Loading BraTS dataset...\")\n        case_paths = glob.glob(os.path.join(DATA_DIR, \"BraTS20_Training_*\"))[:20]  # Use only 20 cases for testing\n        print(f\"Using {len(case_paths)} cases for testing\")\n        \n        # Split data\n        train_paths, val_paths = train_test_split(case_paths, test_size=0.2, random_state=42)\n        \n        # Create optimized generators\n        train_gen = create_optimized_brats_generator(train_paths, batch_size=1, shuffle=True)\n        val_gen = create_optimized_brats_generator(val_paths, batch_size=1, shuffle=False)\n        \n        # Create datasets\n        train_dataset = create_optimized_tf_dataset(train_gen, len(train_paths), batch_size=1)\n        val_dataset = create_optimized_tf_dataset(val_gen, len(val_paths), batch_size=1)\n        \n        # Train model\n        history = train_optimized_model(model, train_dataset, val_dataset, epochs=100)\n        \n        # Save model\n        model.save('optimized_latup_net.h5')\n        print(\"âœ… Optimized model saved!\")\n    \n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T01:48:32.791591Z","iopub.execute_input":"2025-07-14T01:48:32.791848Z","iopub.status.idle":"2025-07-14T01:49:37.623241Z","shell.execute_reply.started":"2025-07-14T01:48:32.791830Z","shell.execute_reply":"2025-07-14T01:49:37.622364Z"}},"outputs":[{"name":"stdout","text":"ğŸ”§ Performance-Optimized LATUP-Net\n==================================================\nCreating optimized model...\nTesting model...\nâœ… Model test passed! Output shape: (1, 64, 64, 64, 4)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"Optimized-LATUP-Net\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Optimized-LATUP-Net\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_2       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\nâ”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)            â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ cast_4 (\u001b[38;5;33mCast\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ input_layer_2[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)            â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ cast_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\nâ”‚                     â”‚ \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)            â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lightweight_parallâ€¦ â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,   â”‚        \u001b[38;5;34m752\u001b[0m â”‚ lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mLightweightParallâ€¦\u001b[0m â”‚ \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m16\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   â”‚            â”‚                   â”‚\nâ”‚                     â”‚ \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)]  â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ optimized_encoder_â€¦ â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,   â”‚     \u001b[38;5;34m14,276\u001b[0m â”‚ lightweight_paraâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mOptimizedEncoderBâ€¦\u001b[0m â”‚ \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   â”‚            â”‚                   â”‚\nâ”‚                     â”‚ \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)]  â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ optimized_encoder_â€¦ â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,   â”‚     \u001b[38;5;34m56,196\u001b[0m â”‚ optimized_encodeâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mOptimizedEncoderBâ€¦\u001b[0m â”‚ \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   â”‚            â”‚                   â”‚\nâ”‚                     â”‚ \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)]     â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv3d_22 (\u001b[38;5;33mConv3D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,   â”‚    \u001b[38;5;34m221,312\u001b[0m â”‚ optimized_encodeâ€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m128\u001b[0m)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,   â”‚        \u001b[38;5;34m512\u001b[0m â”‚ conv3d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m128\u001b[0m)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ leaky_re_lu_17      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mLeakyReLU\u001b[0m)         â”‚ \u001b[38;5;34m128\u001b[0m)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_17          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ leaky_re_lu_17[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚ \u001b[38;5;34m128\u001b[0m)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ optimized_se_blockâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,   â”‚      \u001b[38;5;34m2,184\u001b[0m â”‚ dropout_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”‚ (\u001b[38;5;33mOptimizedSEBlock3â€¦\u001b[0m â”‚ \u001b[38;5;34m128\u001b[0m)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling3d_17    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ optimized_se_bloâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mMaxPooling3D\u001b[0m)      â”‚ \u001b[38;5;34m128\u001b[0m)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ up_sampling3d_8     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ max_pooling3d_17â€¦ â”‚\nâ”‚ (\u001b[38;5;33mUpSampling3D\u001b[0m)      â”‚ \u001b[38;5;34m128\u001b[0m)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling3d_18    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ optimized_encodeâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mMaxPooling3D\u001b[0m)      â”‚ \u001b[38;5;34m64\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ concatenate_20      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ up_sampling3d_8[\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚ \u001b[38;5;34m192\u001b[0m)              â”‚            â”‚ max_pooling3d_18â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv3d_23 (\u001b[38;5;33mConv3D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,   â”‚     \u001b[38;5;34m12,352\u001b[0m â”‚ concatenate_20[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m64\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,   â”‚        \u001b[38;5;34m256\u001b[0m â”‚ conv3d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m64\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ leaky_re_lu_18      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mLeakyReLU\u001b[0m)         â”‚ \u001b[38;5;34m64\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_18          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,   â”‚          \u001b[38;5;34m0\u001b[0m â”‚ leaky_re_lu_18[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚ \u001b[38;5;34m64\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ optimized_se_blockâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m,   â”‚        \u001b[38;5;34m580\u001b[0m â”‚ dropout_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”‚ (\u001b[38;5;33mOptimizedSEBlock3â€¦\u001b[0m â”‚ \u001b[38;5;34m64\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ up_sampling3d_9     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ optimized_se_bloâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mUpSampling3D\u001b[0m)      â”‚ \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling3d_19    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ optimized_encodeâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mMaxPooling3D\u001b[0m)      â”‚ \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ concatenate_21      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ up_sampling3d_9[\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚ \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m96\u001b[0m)           â”‚            â”‚ max_pooling3d_19â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv3d_24 (\u001b[38;5;33mConv3D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚      \u001b[38;5;34m3,104\u001b[0m â”‚ concatenate_21[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚        \u001b[38;5;34m128\u001b[0m â”‚ conv3d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ leaky_re_lu_19      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mLeakyReLU\u001b[0m)         â”‚ \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_19          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ leaky_re_lu_19[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚ \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ optimized_se_blockâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚        \u001b[38;5;34m292\u001b[0m â”‚ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”‚ (\u001b[38;5;33mOptimizedSEBlock3â€¦\u001b[0m â”‚ \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ up_sampling3d_10    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ optimized_se_bloâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mUpSampling3D\u001b[0m)      â”‚ \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling3d_20    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ lightweight_paraâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mMaxPooling3D\u001b[0m)      â”‚ \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ concatenate_22      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ up_sampling3d_10â€¦ â”‚\nâ”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚ \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m48\u001b[0m)           â”‚            â”‚ max_pooling3d_20â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv3d_25 (\u001b[38;5;33mConv3D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚        \u001b[38;5;34m784\u001b[0m â”‚ concatenate_22[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚         \u001b[38;5;34m64\u001b[0m â”‚ conv3d_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ leaky_re_lu_20      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mLeakyReLU\u001b[0m)         â”‚ \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_20          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ leaky_re_lu_20[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mDropout\u001b[0m)           â”‚ \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ optimized_se_blockâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚        \u001b[38;5;34m148\u001b[0m â”‚ dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”‚ (\u001b[38;5;33mOptimizedSEBlock3â€¦\u001b[0m â”‚ \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ up_sampling3d_11    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ optimized_se_bloâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mUpSampling3D\u001b[0m)      â”‚ \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m16\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ cast_5 (\u001b[38;5;33mCast\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ up_sampling3d_11â€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m16\u001b[0m)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv3d_26 (\u001b[38;5;33mConv3D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    â”‚         \u001b[38;5;34m68\u001b[0m â”‚ cast_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\nâ”‚                     â”‚ \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4\u001b[0m)            â”‚            â”‚                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_2       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)            â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ cast_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)            â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ cast_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)            â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lightweight_parallâ€¦ â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,   â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">752</span> â”‚ lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LightweightParallâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   â”‚            â”‚                   â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)]  â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ optimized_encoder_â€¦ â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,   â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,276</span> â”‚ lightweight_paraâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OptimizedEncoderBâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   â”‚            â”‚                   â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)]  â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ optimized_encoder_â€¦ â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,   â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">56,196</span> â”‚ optimized_encodeâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OptimizedEncoderBâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   â”‚            â”‚                   â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]     â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv3d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">221,312</span> â”‚ optimized_encodeâ€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚ conv3d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ leaky_re_lu_17      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_17          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ leaky_re_lu_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ optimized_se_blockâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,184</span> â”‚ dropout_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OptimizedSEBlock3â€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling3d_17    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ optimized_se_bloâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ up_sampling3d_8     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ max_pooling3d_17â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling3D</span>)      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling3d_18    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ optimized_encodeâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ concatenate_20      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ up_sampling3d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)              â”‚            â”‚ max_pooling3d_18â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv3d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> â”‚ concatenate_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ conv3d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ leaky_re_lu_18      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_18          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ leaky_re_lu_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ optimized_se_blockâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>,   â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">580</span> â”‚ dropout_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OptimizedSEBlock3â€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ up_sampling3d_9     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ optimized_se_bloâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling3D</span>)      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling3d_19    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ optimized_encodeâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ concatenate_21      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ up_sampling3d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)           â”‚            â”‚ max_pooling3d_19â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv3d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> â”‚ concatenate_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ conv3d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ leaky_re_lu_19      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_19          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ leaky_re_lu_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ optimized_se_blockâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span> â”‚ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OptimizedSEBlock3â€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ up_sampling3d_10    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ optimized_se_bloâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling3D</span>)      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling3d_20    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ lightweight_paraâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ concatenate_22      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ up_sampling3d_10â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)           â”‚            â”‚ max_pooling3d_20â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv3d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span> â”‚ concatenate_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> â”‚ conv3d_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ leaky_re_lu_20      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_20          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ leaky_re_lu_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ optimized_se_blockâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span> â”‚ dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OptimizedSEBlock3â€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ up_sampling3d_11    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ optimized_se_bloâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling3D</span>)      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ cast_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ up_sampling3d_11â€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)           â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv3d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span> â”‚ cast_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)            â”‚            â”‚                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m313,008\u001b[0m (1.19 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">313,008</span> (1.19 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m312,304\u001b[0m (1.19 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">312,304</span> (1.19 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m704\u001b[0m (2.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> (2.75 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Loading BraTS dataset...\nUsing 20 cases for testing\nğŸš€ Starting Optimized LATUP-Net training...\nâœ… Performance optimizations applied:\n   â€¢ Reduced model complexity\n   â€¢ Mixed precision training\n   â€¢ Smaller input size (64Â³)\n   â€¢ Batch normalization\n   â€¢ Simplified SE blocks\n   â€¢ XLA disabled for faster startup\n   â€¢ GPU memory growth enabled\nEpoch 1/100\n     16/Unknown \u001b[1m34s\u001b[0m 134ms/step - accuracy: 0.2032 - dice_score: 0.2366 - loss: 1.1439\nEpoch 1: val_dice_score improved from -inf to 0.23068, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 424ms/step - accuracy: 0.2092 - dice_score: 0.2376 - loss: 1.1275 - val_accuracy: 0.0048 - val_dice_score: 0.2307 - val_loss: 0.3788 - learning_rate: 0.0010\nEpoch 2/100\n\u001b[1m13/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5334 - dice_score: 0.2976 - loss: 0.2473\nEpoch 2: val_dice_score improved from 0.23068 to 0.24772, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.5482 - dice_score: 0.3000 - loss: 0.2263 - val_accuracy: 0.0048 - val_dice_score: 0.2477 - val_loss: 0.0525 - learning_rate: 0.0010\nEpoch 3/100\n\u001b[1m13/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7142 - dice_score: 0.3426 - loss: -0.0220\nEpoch 3: val_dice_score improved from 0.24772 to 0.26735, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.7175 - dice_score: 0.3442 - loss: -0.0257 - val_accuracy: 0.9237 - val_dice_score: 0.2674 - val_loss: -0.0236 - learning_rate: 0.0010\nEpoch 4/100\n\u001b[1m13/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7934 - dice_score: 0.3818 - loss: -0.0945\nEpoch 4: val_dice_score improved from 0.26735 to 0.29317, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.7932 - dice_score: 0.3835 - loss: -0.0947 - val_accuracy: 0.9931 - val_dice_score: 0.2932 - val_loss: -0.0547 - learning_rate: 0.0010\nEpoch 5/100\n\u001b[1m13/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8162 - dice_score: 0.4217 - loss: -0.1303\nEpoch 5: val_dice_score improved from 0.29317 to 0.32402, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.8169 - dice_score: 0.4235 - loss: -0.1295 - val_accuracy: 0.9931 - val_dice_score: 0.3240 - val_loss: -0.0750 - learning_rate: 0.0010\nEpoch 6/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8406 - dice_score: 0.4668 - loss: -0.1569\nEpoch 6: val_dice_score improved from 0.32402 to 0.35460, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8405 - dice_score: 0.4673 - loss: -0.1565 - val_accuracy: 0.9931 - val_dice_score: 0.3546 - val_loss: -0.0874 - learning_rate: 0.0010\nEpoch 7/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8540 - dice_score: 0.5170 - loss: -0.1770\nEpoch 7: val_dice_score improved from 0.35460 to 0.38696, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8540 - dice_score: 0.5176 - loss: -0.1765 - val_accuracy: 0.9931 - val_dice_score: 0.3870 - val_loss: -0.0994 - learning_rate: 0.0010\nEpoch 8/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8758 - dice_score: 0.5665 - loss: -0.1986\nEpoch 8: val_dice_score improved from 0.38696 to 0.43140, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8762 - dice_score: 0.5671 - loss: -0.1980 - val_accuracy: 0.9931 - val_dice_score: 0.4314 - val_loss: -0.1125 - learning_rate: 0.0010\nEpoch 9/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9161 - dice_score: 0.6270 - loss: -0.2250\nEpoch 9: val_dice_score improved from 0.43140 to 0.49547, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9160 - dice_score: 0.6276 - loss: -0.2242 - val_accuracy: 0.9931 - val_dice_score: 0.4955 - val_loss: -0.1269 - learning_rate: 0.0010\nEpoch 10/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9349 - dice_score: 0.6833 - loss: -0.2450\nEpoch 10: val_dice_score improved from 0.49547 to 0.55141, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9351 - dice_score: 0.6840 - loss: -0.2442 - val_accuracy: 0.9931 - val_dice_score: 0.5514 - val_loss: -0.1393 - learning_rate: 0.0010\nEpoch 11/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9496 - dice_score: 0.7421 - loss: -0.2740\nEpoch 11: val_dice_score improved from 0.55141 to 0.63231, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9499 - dice_score: 0.7428 - loss: -0.2731 - val_accuracy: 0.9931 - val_dice_score: 0.6323 - val_loss: -0.1569 - learning_rate: 0.0010\nEpoch 12/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9634 - dice_score: 0.7954 - loss: -0.3078\nEpoch 12: val_dice_score improved from 0.63231 to 0.68456, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9635 - dice_score: 0.7959 - loss: -0.3067 - val_accuracy: 0.9931 - val_dice_score: 0.6846 - val_loss: -0.1646 - learning_rate: 0.0010\nEpoch 13/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9673 - dice_score: 0.8379 - loss: -0.3354\nEpoch 13: val_dice_score improved from 0.68456 to 0.74583, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9675 - dice_score: 0.8385 - loss: -0.3342 - val_accuracy: 0.9931 - val_dice_score: 0.7458 - val_loss: -0.1725 - learning_rate: 0.0010\nEpoch 14/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9761 - dice_score: 0.8757 - loss: -0.3765\nEpoch 14: val_dice_score improved from 0.74583 to 0.79930, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9763 - dice_score: 0.8762 - loss: -0.3755 - val_accuracy: 0.9931 - val_dice_score: 0.7993 - val_loss: -0.1816 - learning_rate: 0.0010\nEpoch 15/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9795 - dice_score: 0.9060 - loss: -0.4292\nEpoch 15: val_dice_score improved from 0.79930 to 0.85470, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9797 - dice_score: 0.9064 - loss: -0.4276 - val_accuracy: 0.9931 - val_dice_score: 0.8547 - val_loss: -0.1790 - learning_rate: 0.0010\nEpoch 16/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9836 - dice_score: 0.9267 - loss: -0.4533\nEpoch 16: val_dice_score improved from 0.85470 to 0.88107, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9837 - dice_score: 0.9269 - loss: -0.4515 - val_accuracy: 0.9931 - val_dice_score: 0.8811 - val_loss: -0.1738 - learning_rate: 0.0010\nEpoch 17/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9846 - dice_score: 0.9422 - loss: -0.4749\nEpoch 17: val_dice_score improved from 0.88107 to 0.92329, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9847 - dice_score: 0.9424 - loss: -0.4745 - val_accuracy: 0.9931 - val_dice_score: 0.9233 - val_loss: -0.1847 - learning_rate: 0.0010\nEpoch 18/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9866 - dice_score: 0.9536 - loss: -0.5399\nEpoch 18: val_dice_score improved from 0.92329 to 0.92876, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9867 - dice_score: 0.9538 - loss: -0.5393 - val_accuracy: 0.9931 - val_dice_score: 0.9288 - val_loss: -0.1800 - learning_rate: 0.0010\nEpoch 19/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9873 - dice_score: 0.9619 - loss: -0.5819\nEpoch 19: val_dice_score improved from 0.92876 to 0.95850, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9874 - dice_score: 0.9621 - loss: -0.5810 - val_accuracy: 0.9931 - val_dice_score: 0.9585 - val_loss: -0.1788 - learning_rate: 0.0010\nEpoch 20/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9877 - dice_score: 0.9680 - loss: -0.5869\nEpoch 20: val_dice_score improved from 0.95850 to 0.97496, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9877 - dice_score: 0.9681 - loss: -0.5856 - val_accuracy: 0.9931 - val_dice_score: 0.9750 - val_loss: -0.1692 - learning_rate: 0.0010\nEpoch 21/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9882 - dice_score: 0.9728 - loss: -0.6012\nEpoch 21: val_dice_score improved from 0.97496 to 0.98089, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9883 - dice_score: 0.9729 - loss: -0.6011 - val_accuracy: 0.9931 - val_dice_score: 0.9809 - val_loss: -0.1620 - learning_rate: 0.0010\nEpoch 22/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9888 - dice_score: 0.9757 - loss: -0.6368\nEpoch 22: val_dice_score improved from 0.98089 to 0.98874, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9889 - dice_score: 0.9759 - loss: -0.6372 - val_accuracy: 0.9931 - val_dice_score: 0.9887 - val_loss: -0.1726 - learning_rate: 0.0010\nEpoch 23/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9889 - dice_score: 0.9782 - loss: -0.6664\nEpoch 23: val_dice_score improved from 0.98874 to 0.98981, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9890 - dice_score: 0.9783 - loss: -0.6657 - val_accuracy: 0.9931 - val_dice_score: 0.9898 - val_loss: -0.1624 - learning_rate: 0.0010\nEpoch 24/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9883 - dice_score: 0.9791 - loss: -0.6211\nEpoch 24: val_dice_score improved from 0.98981 to 0.99005, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9884 - dice_score: 0.9792 - loss: -0.6229 - val_accuracy: 0.9931 - val_dice_score: 0.9901 - val_loss: -0.1664 - learning_rate: 0.0010\nEpoch 25/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9888 - dice_score: 0.9807 - loss: -0.6675\nEpoch 25: val_dice_score did not improve from 0.99005\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9890 - dice_score: 0.9808 - loss: -0.6678 - val_accuracy: 0.9931 - val_dice_score: 0.9832 - val_loss: -0.1625 - learning_rate: 0.0010\nEpoch 26/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9894 - dice_score: 0.9821 - loss: -0.6796\nEpoch 26: val_dice_score did not improve from 0.99005\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9895 - dice_score: 0.9822 - loss: -0.6792 - val_accuracy: 0.9931 - val_dice_score: 0.9872 - val_loss: -0.1488 - learning_rate: 0.0010\nEpoch 27/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9891 - dice_score: 0.9825 - loss: -0.6589\nEpoch 27: val_dice_score improved from 0.99005 to 0.99006, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9892 - dice_score: 0.9826 - loss: -0.6608 - val_accuracy: 0.9931 - val_dice_score: 0.9901 - val_loss: -0.1748 - learning_rate: 0.0010\nEpoch 28/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9896 - dice_score: 0.9830 - loss: -0.7365\nEpoch 28: val_dice_score improved from 0.99006 to 0.99067, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9897 - dice_score: 0.9831 - loss: -0.7372 - val_accuracy: 0.9931 - val_dice_score: 0.9907 - val_loss: -0.2094 - learning_rate: 0.0010\nEpoch 29/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9899 - dice_score: 0.9845 - loss: -0.7533\nEpoch 29: val_dice_score improved from 0.99067 to 0.99077, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9900 - dice_score: 0.9846 - loss: -0.7534 - val_accuracy: 0.9931 - val_dice_score: 0.9908 - val_loss: -0.1900 - learning_rate: 0.0010\nEpoch 30/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9903 - dice_score: 0.9853 - loss: -0.7838\nEpoch 30: val_dice_score did not improve from 0.99077\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9903 - dice_score: 0.9854 - loss: -0.7839 - val_accuracy: 0.9931 - val_dice_score: 0.9884 - val_loss: -0.1884 - learning_rate: 0.0010\nEpoch 31/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9895 - dice_score: 0.9841 - loss: -0.7641\nEpoch 31: val_dice_score did not improve from 0.99077\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9896 - dice_score: 0.9842 - loss: -0.7639 - val_accuracy: 0.9937 - val_dice_score: 0.9896 - val_loss: -0.2522 - learning_rate: 0.0010\nEpoch 32/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9900 - dice_score: 0.9854 - loss: -0.7778\nEpoch 32: val_dice_score did not improve from 0.99077\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9901 - dice_score: 0.9855 - loss: -0.7782 - val_accuracy: 0.9944 - val_dice_score: 0.9888 - val_loss: -0.2964 - learning_rate: 0.0010\nEpoch 33/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9899 - dice_score: 0.9855 - loss: -0.7538\nEpoch 33: val_dice_score improved from 0.99077 to 0.99084, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9900 - dice_score: 0.9856 - loss: -0.7566 - val_accuracy: 0.9944 - val_dice_score: 0.9908 - val_loss: -0.3212 - learning_rate: 0.0010\nEpoch 34/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9901 - dice_score: 0.9861 - loss: -0.8131\nEpoch 34: val_dice_score improved from 0.99084 to 0.99285, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9902 - dice_score: 0.9862 - loss: -0.8137 - val_accuracy: 0.9931 - val_dice_score: 0.9928 - val_loss: -0.1883 - learning_rate: 0.0010\nEpoch 35/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9901 - dice_score: 0.9868 - loss: -0.8254\nEpoch 35: val_dice_score did not improve from 0.99285\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9902 - dice_score: 0.9869 - loss: -0.8263 - val_accuracy: 0.9931 - val_dice_score: 0.9914 - val_loss: -0.2034 - learning_rate: 0.0010\nEpoch 36/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9901 - dice_score: 0.9869 - loss: -0.8314\nEpoch 36: val_dice_score improved from 0.99285 to 0.99304, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9902 - dice_score: 0.9870 - loss: -0.8318 - val_accuracy: 0.9931 - val_dice_score: 0.9930 - val_loss: -0.1853 - learning_rate: 0.0010\nEpoch 37/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9897 - dice_score: 0.9868 - loss: -0.8202\nEpoch 37: val_dice_score did not improve from 0.99304\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9898 - dice_score: 0.9869 - loss: -0.8212 - val_accuracy: 0.9931 - val_dice_score: 0.9930 - val_loss: -0.1928 - learning_rate: 0.0010\nEpoch 38/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9899 - dice_score: 0.9872 - loss: -0.8054\nEpoch 38: val_dice_score did not improve from 0.99304\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9900 - dice_score: 0.9873 - loss: -0.8068 - val_accuracy: 0.9931 - val_dice_score: 0.9916 - val_loss: -0.1915 - learning_rate: 0.0010\nEpoch 39/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9899 - dice_score: 0.9871 - loss: -0.8387\nEpoch 39: val_dice_score improved from 0.99304 to 0.99376, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9900 - dice_score: 0.9873 - loss: -0.8400 - val_accuracy: 0.9946 - val_dice_score: 0.9938 - val_loss: -0.3301 - learning_rate: 0.0010\nEpoch 40/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9903 - dice_score: 0.9877 - loss: -0.8822\nEpoch 40: val_dice_score did not improve from 0.99376\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9904 - dice_score: 0.9878 - loss: -0.8832 - val_accuracy: 0.9947 - val_dice_score: 0.9937 - val_loss: -0.3740 - learning_rate: 0.0010\nEpoch 41/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9908 - dice_score: 0.9884 - loss: -0.8891\nEpoch 41: val_dice_score did not improve from 0.99376\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9908 - dice_score: 0.9885 - loss: -0.8894 - val_accuracy: 0.9936 - val_dice_score: 0.9937 - val_loss: -0.2385 - learning_rate: 0.0010\nEpoch 42/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9902 - dice_score: 0.9881 - loss: -0.8276\nEpoch 42: val_dice_score improved from 0.99376 to 0.99455, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9903 - dice_score: 0.9882 - loss: -0.8290 - val_accuracy: 0.9951 - val_dice_score: 0.9946 - val_loss: -0.4282 - learning_rate: 0.0010\nEpoch 43/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9902 - dice_score: 0.9880 - loss: -0.8697\nEpoch 43: val_dice_score did not improve from 0.99455\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9903 - dice_score: 0.9881 - loss: -0.8707 - val_accuracy: 0.9931 - val_dice_score: 0.9931 - val_loss: -0.1838 - learning_rate: 0.0010\nEpoch 44/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9903 - dice_score: 0.9886 - loss: -0.9033\nEpoch 44: val_dice_score did not improve from 0.99455\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9904 - dice_score: 0.9887 - loss: -0.9041 - val_accuracy: 0.9935 - val_dice_score: 0.9936 - val_loss: -0.2467 - learning_rate: 0.0010\nEpoch 45/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9906 - dice_score: 0.9888 - loss: -0.9104\nEpoch 45: val_dice_score did not improve from 0.99455\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9907 - dice_score: 0.9889 - loss: -0.9108 - val_accuracy: 0.9931 - val_dice_score: 0.9931 - val_loss: -0.1718 - learning_rate: 0.0010\nEpoch 46/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9906 - dice_score: 0.9891 - loss: -0.8615\nEpoch 46: val_dice_score did not improve from 0.99455\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9907 - dice_score: 0.9892 - loss: -0.8625 - val_accuracy: 0.9933 - val_dice_score: 0.9933 - val_loss: -0.1880 - learning_rate: 0.0010\nEpoch 47/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9910 - dice_score: 0.9894 - loss: -0.8891\nEpoch 47: val_dice_score did not improve from 0.99455\n\nEpoch 47: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9911 - dice_score: 0.9895 - loss: -0.8904 - val_accuracy: 0.9932 - val_dice_score: 0.9932 - val_loss: -0.1743 - learning_rate: 0.0010\nEpoch 48/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9908 - dice_score: 0.9893 - loss: -0.8935\nEpoch 48: val_dice_score did not improve from 0.99455\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9909 - dice_score: 0.9895 - loss: -0.8941 - val_accuracy: 0.9934 - val_dice_score: 0.9935 - val_loss: -0.2384 - learning_rate: 5.0000e-04\nEpoch 49/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9889 - dice_score: 0.9873 - loss: -0.8948\nEpoch 49: val_dice_score did not improve from 0.99455\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9891 - dice_score: 0.9875 - loss: -0.8978 - val_accuracy: 0.9938 - val_dice_score: 0.9938 - val_loss: -0.2727 - learning_rate: 5.0000e-04\nEpoch 50/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9907 - dice_score: 0.9895 - loss: -0.9572\nEpoch 50: val_dice_score did not improve from 0.99455\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9909 - dice_score: 0.9896 - loss: -0.9590 - val_accuracy: 0.9936 - val_dice_score: 0.9936 - val_loss: -0.2666 - learning_rate: 5.0000e-04\nEpoch 51/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9913 - dice_score: 0.9900 - loss: -0.9931\nEpoch 51: val_dice_score did not improve from 0.99455\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9914 - dice_score: 0.9901 - loss: -0.9941 - val_accuracy: 0.9945 - val_dice_score: 0.9944 - val_loss: -0.3564 - learning_rate: 5.0000e-04\nEpoch 52/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9910 - dice_score: 0.9896 - loss: -0.9885\nEpoch 52: val_dice_score improved from 0.99455 to 0.99475, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9911 - dice_score: 0.9897 - loss: -0.9894 - val_accuracy: 0.9948 - val_dice_score: 0.9947 - val_loss: -0.4862 - learning_rate: 5.0000e-04\nEpoch 53/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9912 - dice_score: 0.9898 - loss: -0.9659\nEpoch 53: val_dice_score did not improve from 0.99475\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9913 - dice_score: 0.9899 - loss: -0.9670 - val_accuracy: 0.9948 - val_dice_score: 0.9933 - val_loss: -0.4986 - learning_rate: 5.0000e-04\nEpoch 54/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9910 - dice_score: 0.9897 - loss: -0.9779\nEpoch 54: val_dice_score did not improve from 0.99475\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9911 - dice_score: 0.9899 - loss: -0.9796 - val_accuracy: 0.9801 - val_dice_score: 0.9776 - val_loss: -0.4868 - learning_rate: 5.0000e-04\nEpoch 55/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9909 - dice_score: 0.9898 - loss: -0.9880\nEpoch 55: val_dice_score did not improve from 0.99475\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9910 - dice_score: 0.9899 - loss: -0.9899 - val_accuracy: 0.9946 - val_dice_score: 0.9944 - val_loss: -0.4262 - learning_rate: 5.0000e-04\nEpoch 56/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9911 - dice_score: 0.9901 - loss: -0.9857\nEpoch 56: val_dice_score did not improve from 0.99475\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9912 - dice_score: 0.9902 - loss: -0.9875 - val_accuracy: 0.9927 - val_dice_score: 0.9916 - val_loss: -0.4179 - learning_rate: 5.0000e-04\nEpoch 57/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9915 - dice_score: 0.9903 - loss: -1.0028\nEpoch 57: val_dice_score did not improve from 0.99475\n\nEpoch 57: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9916 - dice_score: 0.9905 - loss: -1.0042 - val_accuracy: 0.9871 - val_dice_score: 0.9841 - val_loss: -0.4960 - learning_rate: 5.0000e-04\nEpoch 58/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9916 - dice_score: 0.9905 - loss: -1.0307\nEpoch 58: val_dice_score did not improve from 0.99475\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9917 - dice_score: 0.9906 - loss: -1.0321 - val_accuracy: 0.9941 - val_dice_score: 0.9933 - val_loss: -0.4426 - learning_rate: 2.5000e-04\nEpoch 59/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9918 - dice_score: 0.9908 - loss: -1.0344\nEpoch 59: val_dice_score did not improve from 0.99475\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9919 - dice_score: 0.9909 - loss: -1.0358 - val_accuracy: 0.9934 - val_dice_score: 0.9920 - val_loss: -0.4479 - learning_rate: 2.5000e-04\nEpoch 60/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9920 - dice_score: 0.9911 - loss: -1.0455\nEpoch 60: val_dice_score did not improve from 0.99475\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9921 - dice_score: 0.9912 - loss: -1.0468 - val_accuracy: 0.9929 - val_dice_score: 0.9920 - val_loss: -0.3419 - learning_rate: 2.5000e-04\nEpoch 61/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9918 - dice_score: 0.9907 - loss: -1.0403\nEpoch 61: val_dice_score did not improve from 0.99475\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9919 - dice_score: 0.9909 - loss: -1.0418 - val_accuracy: 0.9942 - val_dice_score: 0.9933 - val_loss: -0.4767 - learning_rate: 2.5000e-04\nEpoch 62/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9919 - dice_score: 0.9910 - loss: -1.0526\nEpoch 62: val_dice_score improved from 0.99475 to 0.99488, saving model to best_optimized_latup.weights.h5\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9920 - dice_score: 0.9911 - loss: -1.0539 - val_accuracy: 0.9953 - val_dice_score: 0.9949 - val_loss: -0.6187 - learning_rate: 2.5000e-04\nEpoch 63/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9921 - dice_score: 0.9910 - loss: -1.0555\nEpoch 63: val_dice_score did not improve from 0.99488\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9922 - dice_score: 0.9911 - loss: -1.0567 - val_accuracy: 0.9747 - val_dice_score: 0.9728 - val_loss: -0.4839 - learning_rate: 2.5000e-04\nEpoch 64/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9920 - dice_score: 0.9911 - loss: -1.0652\nEpoch 64: val_dice_score did not improve from 0.99488\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9921 - dice_score: 0.9912 - loss: -1.0667 - val_accuracy: 0.9938 - val_dice_score: 0.9919 - val_loss: -0.5749 - learning_rate: 2.5000e-04\nEpoch 65/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9923 - dice_score: 0.9915 - loss: -1.0675\nEpoch 65: val_dice_score did not improve from 0.99488\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9924 - dice_score: 0.9916 - loss: -1.0687 - val_accuracy: 0.9948 - val_dice_score: 0.9948 - val_loss: -0.4918 - learning_rate: 2.5000e-04\nEpoch 66/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9924 - dice_score: 0.9915 - loss: -1.0686\nEpoch 66: val_dice_score did not improve from 0.99488\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9925 - dice_score: 0.9916 - loss: -1.0697 - val_accuracy: 0.9921 - val_dice_score: 0.9906 - val_loss: -0.5100 - learning_rate: 2.5000e-04\nEpoch 67/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9926 - dice_score: 0.9916 - loss: -1.0781\nEpoch 67: val_dice_score did not improve from 0.99488\n\nEpoch 67: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9927 - dice_score: 0.9917 - loss: -1.0792 - val_accuracy: 0.9829 - val_dice_score: 0.9817 - val_loss: -0.5415 - learning_rate: 2.5000e-04\nEpoch 68/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9921 - dice_score: 0.9912 - loss: -1.0571\nEpoch 68: val_dice_score did not improve from 0.99488\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9922 - dice_score: 0.9913 - loss: -1.0588 - val_accuracy: 0.9532 - val_dice_score: 0.9490 - val_loss: -0.4181 - learning_rate: 1.2500e-04\nEpoch 69/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9924 - dice_score: 0.9916 - loss: -1.0780\nEpoch 69: val_dice_score did not improve from 0.99488\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9925 - dice_score: 0.9917 - loss: -1.0792 - val_accuracy: 0.9919 - val_dice_score: 0.9899 - val_loss: -0.6549 - learning_rate: 1.2500e-04\nEpoch 70/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9925 - dice_score: 0.9917 - loss: -1.0915\nEpoch 70: val_dice_score did not improve from 0.99488\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9926 - dice_score: 0.9918 - loss: -1.0929 - val_accuracy: 0.9941 - val_dice_score: 0.9926 - val_loss: -0.6812 - learning_rate: 1.2500e-04\nEpoch 71/100\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9926 - dice_score: 0.9918 - loss: -1.0938\nEpoch 71: val_dice_score did not improve from 0.99488\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9927 - dice_score: 0.9919 - loss: -1.0950 - val_accuracy: 0.9920 - val_dice_score: 0.9904 - val_loss: -0.6548 - learning_rate: 1.2500e-04\nEpoch 72/100\n\u001b[1m15/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9925 - dice_score: 0.9918 - loss: -1.1033\nEpoch 72: val_dice_score did not improve from 0.99488\n\nEpoch 72: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9927 - dice_score: 0.9920 - loss: -1.1047 - val_accuracy: 0.9912 - val_dice_score: 0.9898 - val_loss: -0.5774 - learning_rate: 1.2500e-04\nEpoch 72: early stopping\nRestoring model weights from the end of the best epoch: 62.\nâœ… Optimized model saved!\n","output_type":"stream"}],"execution_count":3}]}